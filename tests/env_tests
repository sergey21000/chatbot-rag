GRADIO_SERVER_PORT=7860
GRADIO_DEBUG=0
GRADIO_ANALYTICS_ENABLED=False
# GRADIO_SHARE=True

LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF:q4_k_m
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-0.6B-GGUF/resolve/main/Qwen_Qwen3-0.6B-Q4_K_M.gguf

# LLAMA_ARG_HF_REPO=Qwen/Qwen3-VL-2B-Instruct-GGUF:q4_k_m
# LLAMA_ARG_MMPROJ_AUTO=0

LLAMA_ARG_JINJA=1
LLAMA_ARG_CTX_SIZE=4096
LLAMA_ARG_N_PARALLEL=1
LLAMA_LOG_VERBOSITY=1
LLAMA_LOG_COLORS=auto
LLAMA_ARG_NO_WEBUI=1
LLAMA_CACHE=llm_models

LLAMA_ARG_PORT=8081
LLAMA_ARG_HOST=127.0.0.1

EMBED_MODEL_REPO=sergeyzh/rubert-tiny-turbo

LLAMACPP_SERVER_TIMEOUT_WAIT=1500

# LLAMACPP_RELEASE_ZIP_URL=https://github.com/ggml-org/llama.cpp/releases/download/b7806/llama-b7806-bin-ubuntu-x64.tar.gz
LLAMACPP_RELEASE_TAG=b7806

CHATBOT_LOG_LEVEL=DEBUG
CHATBOT_RAG_ENABLED=1
