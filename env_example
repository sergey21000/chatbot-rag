# ==============================================================
# Gradio Environment Variables
# https://www.gradio.app/main/guides/environment-variables
# ==============================================================

GRADIO_SERVER_PORT=7860
# GRADIO_SERVER_NAME=127.0.0.1
# GRADIO_SERVER_NAME=0.0.0.0

# GRADIO_DEBUG=0
# GRADIO_ANALYTICS_ENABLED=False
# GRADIO_SHARE=True
# GRADIO_TEMP_DIR=gradio_temp


# ==============================================================
# llama.cpp Environment Variables
# https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md
# ==============================================================

# ==============================================================
# llama.cpp выбор LLM модели

# вариант 1 (для данного чат бота не ставить - не подходит)
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/Qwen_Qwen3-0.6B-GGUF/resolve/main/Qwen_Qwen3-0.6B-Q4_K_M.gguf

# вариант 2
# LLAMA_ARG_MODEL=llm_models/bartowski_google_gemma-3-1b-it-GGUF_google_gemma-3-1b-it-Q8_0.gguf

# вариант 3
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF:q4_k_m
LLAMA_ARG_HF_REPO=bartowski/google_gemma-3-1b-it-GGUF:q8_0

# вариант 4
# LLAMA_ARG_HF_REPO=bartowski/Qwen_Qwen3-0.6B-GGUF
# LLAMA_ARG_HF_FILE=Qwen_Qwen3-0.6B-Q4_K_M.gguf

# ==============================================================
# llama.cpp выбор VLLM модели

# вариант 1
# gemma-3-4b
# LLAMA_ARG_MODEL_URL=https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/mmproj-google_gemma-3-4b-it-f16.gguf

# qwen3-vl 4B
# LLAMA_ARG_MODEL_URL=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3VL-4B-Instruct-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-4B-Instruct-Q8_0.gguf

# qwen3-vl 2B
# LLAMA_ARG_MODEL_URL=https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3VL-2B-Instruct-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-2B-Instruct-Q8_0.gguf

# вариант 2
# LLAMA_ARG_MODEL=D:/models/bartowski_google_gemma-3-4b-it-GGUF_google_gemma-3-4b-it-Q4_K_M.gguf
# LLAMA_ARG_MMPROJ=D:/models/mmproj-google_gemma-3-4b-it-f16.gguf

# вариант 3 (mmproj загружается автоматически если доступен)
# LLAMA_ARG_HF_REPO=Qwen/Qwen3-VL-2B-Instruct-GGUF:q4_k_m
# LLAMA_ARG_HF_REPO=Qwen/Qwen3-VL-4B-Instruct-GGUF:q4_k_m
# LLAMA_ARG_HF_REPO=bartowski/google_gemma-3-4b-it-GGUF:q4_k_m

# отключить автоматическую загрузку файла mmproj (по умолчанию: 1)
# LLAMA_ARG_MMPROJ_AUTO=0

# ==============================================================
# llama.cpp настройки

LLAMA_ARG_JINJA=1
LLAMA_ARG_CTX_SIZE=4096
LLAMA_ARG_N_PARALLEL=1
LLAMA_ARG_N_GPU_LAYERS=-1

LLAMA_LOG_VERBOSITY=3
LLAMA_LOG_COLORS=auto

LLAMA_ARG_NO_WEBUI=1
LLAMA_CACHE=llm_models

# ==============================================================
# llama.cpp сервер и порт

LLAMA_ARG_PORT=8081
LLAMA_ARG_HOST=0.0.0.0


# ==============================================================
# HF Environment Variables
# https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables
# ==============================================================

# HF_TOKEN=""


# ==============================================================
# Other Environment Variables
# ==============================================================

# модель эмбедингов
EMBED_MODEL_REPO=Alibaba-NLP/gte-multilingual-base

# увеличение ожидания запуска сервера llama.cpp
LLAMACPP_SERVER_TIMEOUT_WAIT=900

# прямая ссылка на релиз llama.cpp
# https://github.com/ggml-org/llama.cpp/releases
# LLAMACPP_RELEASE_ZIP_URL=https://github.com/ggml-org/llama.cpp/releases/download/b7806/llama-b7806-bin-win-cuda-13.1-x64.zip

# установка тега релиза llama.cpp вручную
# LLAMACPP_RELEASE_TAG=b7806

# путь до предварительно скомпилированной llama.cpp
# LLAMACPP_DIR=""

# прямая ссылка на llm модель вместо llama.cpp если таковая используется  
# OPENAI_BASE_URL=""

# включить или отключить режим RAG
CHATBOT_RAG_ENABLED=1

# установить уровень логгирования для отладки
# CHATBOT_LOG_LEVEL=DEBUG
